{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decoding-Data-Science/nov25/blob/main/energy_plant_visual_ai_clip_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2bceae5",
      "metadata": {
        "id": "a2bceae5"
      },
      "source": [
        "# Energy Plant Visual AI Demo â€“ CLIP on Hugging Face\n",
        "\n",
        "In this project, we extend the **energy plant** use case into **Visual AI** using an openâ€‘source model from Hugging Face.\n",
        "\n",
        "We will:\n",
        "- Use the **OpenAI CLIP** visionâ€“language model (via Hugging Face: `openai/clip-vit-base-patch32`)  \n",
        "- Take a **photo of an energy plant component** (e.g., turbine, valve, control panel, switchgear)  \n",
        "- Ask the model to **classify what the picture is about** using zeroâ€‘shot learning  \n",
        "- Build a small **labelling pipeline** that can be used for:\n",
        "  - Quickly tagging plant images (component type, area, risk category)  \n",
        "  - Preparing data for supervised training later  \n",
        "  - Demonstrating Visual AI in an MLOps / monitoring context\n",
        "\n",
        "> ðŸ’¡ **Workshop goal:**  \n",
        "> Show students how they can go from **raw images** â†’ **preâ€‘trained openâ€‘source model** â†’ **meaningful labels** without training a deep network from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8175d725",
      "metadata": {
        "id": "8175d725"
      },
      "outputs": [],
      "source": [
        "# 1. Install dependencies (run this once in Colab or your environment)\n",
        "# If you're running locally and already have these installed, you can skip this cell.\n",
        "\n",
        "!pip install -q transformers==4.46.0 torch pillow matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce7239af",
      "metadata": {
        "id": "ce7239af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\almehairbi\\Desktop\\Dell AI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\almehairbi\\Desktop\\Dell AI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\almehairbi\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: openai/clip-vit-base-patch32\n"
          ]
        }
      ],
      "source": [
        "# 2. Imports and configuration\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load CLIP model & processor from Hugging Face\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(\"Model loaded:\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a1535c",
      "metadata": {
        "id": "b2a1535c"
      },
      "source": [
        "## 3. Understanding CLIP in Simple Terms\n",
        "\n",
        "**CLIP** (Contrastive Languageâ€“Image Preâ€‘training) is a model that:\n",
        "\n",
        "- Takes an **image** and a set of **text prompts** (e.g., *\"a photo of a turbine\"*, *\"a photo of a control panel\"*).  \n",
        "- Embeds both into a shared space and computes **similarity scores**.  \n",
        "- The text whose embedding is closest to the image is considered the **best label**.\n",
        "\n",
        "We will use CLIP in a **zeroâ€‘shot** way:\n",
        "\n",
        "- No extra training on our side.  \n",
        "- We only define **good, descriptive prompts** for the different plant components / conditions we care about.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "61b15d62",
      "metadata": {
        "id": "61b15d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a photo of a turbine in an industrial energy plant\n",
            "a photo of a control room in an industrial energy plant\n",
            "a photo of a control panel in an industrial energy plant\n",
            "a photo of a switchgear in an industrial energy plant\n",
            "a photo of a valve in an industrial energy plant\n",
            "a photo of a pipeline in an industrial energy plant\n",
            "a photo of a cooling tower in an industrial energy plant\n",
            "a photo of a boiler area in an industrial energy plant\n",
            "a photo of a pump in an industrial energy plant\n",
            "a photo of a electrical cabinet in an industrial energy plant\n"
          ]
        }
      ],
      "source": [
        "# 4. Define candidate labels for the energy plant\n",
        "\n",
        "# Plain labels (for logging / readability)\n",
        "plain_labels = [\n",
        "    \"turbine\",\n",
        "    \"control room\",\n",
        "    \"control panel\",\n",
        "    \"switchgear\",\n",
        "    \"valve\",\n",
        "    \"pipeline\",\n",
        "    \"cooling tower\",\n",
        "    \"boiler area\",\n",
        "    \"pump\",\n",
        "    \"electrical cabinet\",\n",
        "]\n",
        "\n",
        "# Text prompts for CLIP â€“ more descriptive usually works better\n",
        "text_prompts = [f\"a photo of a {lbl} in an industrial energy plant\" for lbl in plain_labels]\n",
        "\n",
        "for p in text_prompts:\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2a9ae274",
      "metadata": {
        "id": "2a9ae274"
      },
      "outputs": [],
      "source": [
        "# 5. Helper functions: load image, classify, and visualize\n",
        "\n",
        "def load_image(image_path: str) -> Image.Image:\n",
        "    \"\"\"Load an image from a local path.\"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def classify_image_with_clip(\n",
        "    image: Image.Image,\n",
        "    text_prompts: List[str],\n",
        "    model: CLIPModel,\n",
        "    processor: CLIPProcessor,\n",
        "    device: str = \"cpu\",\n",
        "    top_k: int = 5,\n",
        "):\n",
        "    \"\"\"Run CLIP on a single image + list of text prompts and return top-k scores.\"\"\"\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(\n",
        "        text=text_prompts,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # CLIP returns logits_per_image: [batch_size, num_text_prompts]\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy().flatten()\n",
        "\n",
        "    # Sort prompts by probability\n",
        "    indices = probs.argsort()[::-1]  # descending\n",
        "    top_indices = indices[:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"prompt\": text_prompts[idx],\n",
        "            \"probability\": float(probs[idx]),\n",
        "            \"plain_label\": plain_labels[idx],\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def show_image_with_predictions(image: Image.Image, predictions):\n",
        "    \"\"\"Display image with top-1 prediction in the title and print top-k table.\"\"\"\n",
        "    top1 = predictions[0]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Top-1: {top1['plain_label']} ({top1['probability']:.2%})\")\n",
        "    plt.show()\n",
        "\n",
        "    # Display as a small table\n",
        "    df = pd.DataFrame(predictions)\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82371169",
      "metadata": {
        "id": "82371169"
      },
      "source": [
        "## 6. Providing Images for the Demo\n",
        "\n",
        "You have a few options for getting energy plant images into this notebook:\n",
        "\n",
        "1. **Upload manually in Colab**  \n",
        "   - Go to: `Files` panel â†’ Upload your `turbine.jpg`, `valve.jpg`, etc.  \n",
        "   - Or use a small upload cell (shown next).  \n",
        "\n",
        "2. **Use a local folder** (if running in VS Code / Jupyter locally)  \n",
        "   - Put images under a folder like `data/plant_images/`.  \n",
        "   - Refer to them by their relative path.  \n",
        "\n",
        "For the workshop, you can prepare a **small image pack** (5â€“10 photos) and share it with students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b68130",
      "metadata": {
        "id": "a0b68130"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Colab upload helper\n",
        "# Uncomment and run this cell if you're using Google Colab and want to upload files manually.\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # This lets you pick images from your computer\n",
        "# list(uploaded.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdf2548",
      "metadata": {
        "id": "bfdf2548"
      },
      "outputs": [],
      "source": [
        "# 7. Single-image classification demo\n",
        "\n",
        "# Replace this path with one of your uploaded images, e.g. \"turbine_1.jpg\"\n",
        "example_image_path = \"C:\\\\Users\\\\almehairbi\\\\Desktop\\\\Dell AI\\\\nov25\\\\data\\\\turbine1.jpg\"  # <- change this\n",
        "\n",
        "try:\n",
        "    img = load_image(example_image_path)\n",
        "    preds = classify_image_with_clip(img, text_prompts, model, processor, device=device, top_k=5)\n",
        "    show_image_with_predictions(img, preds)\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    print(\"Please update `example_image_path` to point to a real file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3551f3c6",
      "metadata": {
        "id": "3551f3c6"
      },
      "source": [
        "### What just happened?\n",
        "\n",
        "1. We loaded an **energy plant image** (or a placeholder image if you pointed to another photo).  \n",
        "2. We sent the image + a list of **text prompts** (plant components) to CLIP.  \n",
        "3. CLIP computed **similarity scores** and we converted them to probabilities with `softmax`.  \n",
        "4. We displayed the **topâ€‘k labels** (e.g., *turbine*, *valve*, *control panel*) with their probabilities.\n",
        "\n",
        "> This is already a **useful Visual AI tool**: you can drag & drop a new plant photo and instantly get a suggested label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30ca0b9",
      "metadata": {
        "id": "e30ca0b9"
      },
      "outputs": [],
      "source": [
        "# 8. Batch scoring for a folder of images\n",
        "\n",
        "def classify_folder(\n",
        "    folder_path: str,\n",
        "    text_prompts: List[str],\n",
        "    model: CLIPModel,\n",
        "    processor: CLIPProcessor,\n",
        "    device: str = \"cpu\",\n",
        "    top_k: int = 3,\n",
        "):\n",
        "    \"\"\"Classify all images in a folder and return a DataFrame of results.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    if not os.path.isdir(folder_path):\n",
        "        raise NotADirectoryError(f\"Folder not found: {folder_path}\")\n",
        "\n",
        "    image_files = [\n",
        "        f for f in os.listdir(folder_path)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
        "    ]\n",
        "\n",
        "    for fname in image_files:\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        img = load_image(fpath)\n",
        "        preds = classify_image_with_clip(img, text_prompts, model, processor, device=device, top_k=top_k)\n",
        "\n",
        "        top1 = preds[0]\n",
        "        records.append({\n",
        "            \"filename\": fname,\n",
        "            \"top1_label\": top1[\"plain_label\"],\n",
        "            \"top1_prob\": top1[\"probability\"],\n",
        "            \"all_predictions\": preds,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage (update the folder name)\n",
        "folder = \"plant_images\"  # e.g. a folder with turbine/valve/etc. photos\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    batch_df = classify_folder(folder, text_prompts, model, processor, device=device, top_k=3)\n",
        "    display(batch_df[[\"filename\", \"top1_label\", \"top1_prob\"]])\n",
        "else:\n",
        "    print(f\"Folder '{folder}' not found. Create it and add images, or change the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e67236b6",
      "metadata": {
        "id": "e67236b6"
      },
      "source": [
        "## 9. Optional: Add Ground Truth Labels and Measure Accuracy\n",
        "\n",
        "If you want to turn this into a **proper miniâ€‘project**, you can:\n",
        "\n",
        "1. Create a CSV file, e.g. `labels.csv`, with columns:\n",
        "   - `filename` â€“ image file name  \n",
        "   - `true_label` â€“ the real component type (e.g., `turbine`, `valve`)  \n",
        "\n",
        "2. After running the batch classification, **join** predictions with this CSV and measure accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PUpuAZcHgjUm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PUpuAZcHgjUm",
        "outputId": "3c87b585-097c-4c22-bd75-e4ec128eb6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 images in 'plant_images'\n",
            "Saved template labels file to labels.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-82bf9161-7df3-4ffd-b8ea-bb5099d5590d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>true_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82bf9161-7df3-4ffd-b8ea-bb5099d5590d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82bf9161-7df3-4ffd-b8ea-bb5099d5590d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82bf9161-7df3-4ffd-b8ea-bb5099d5590d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [filename, true_label]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Folder where your images are stored\n",
        "folder = \"plant_images\"   # change if needed\n",
        "\n",
        "# Get all image filenames from the folder\n",
        "image_files = [\n",
        "    f for f in os.listdir(folder)\n",
        "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
        "]\n",
        "\n",
        "print(f\"Found {len(image_files)} images in '{folder}'\")\n",
        "\n",
        "# OPTION 1: Create an empty template for you to fill manually\n",
        "labels_df = pd.DataFrame({\n",
        "    \"filename\": image_files,\n",
        "    \"true_label\": \"\"   # fill this later (e.g., turbine, valve, control_panel)\n",
        "})\n",
        "\n",
        "# OPTION 2 (optional): auto-infer label from filename prefix before underscore\n",
        "# e.g. turbine_01.jpg -> 'turbine'\n",
        "# labels_df[\"true_label\"] = labels_df[\"filename\"].str.split(\"_\").str[0]\n",
        "\n",
        "# Save to CSV\n",
        "labels_csv = \"labels.csv\"\n",
        "labels_df.to_csv(labels_csv, index=False)\n",
        "\n",
        "print(f\"Saved template labels file to {labels_csv}\")\n",
        "display(labels_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6cf7bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6cf7bd",
        "outputId": "060551ec-1f5d-4b1e-e764-d9502f655017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Either `batch_df` is not defined yet or `labels.csv` was not found.\n"
          ]
        }
      ],
      "source": [
        "# 9.1 Example: join predictions with ground-truth labels and compute accuracy\n",
        "\n",
        "labels_csv = \"labels.csv\"  # <-- create this file with filename,true_label\n",
        "\n",
        "if \"batch_df\" in globals() and os.path.exists(labels_csv):\n",
        "    gt = pd.read_csv(labels_csv)\n",
        "    merged = batch_df.merge(gt, on=\"filename\", how=\"inner\")\n",
        "\n",
        "    merged[\"correct\"] = merged[\"top1_label\"] == merged[\"true_label\"]\n",
        "    accuracy = merged[\"correct\"].mean()\n",
        "\n",
        "    print(f\"Number of images with labels: {len(merged)}\")\n",
        "    print(f\"Top-1 accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    display(merged[[\"filename\", \"true_label\", \"top1_label\", \"top1_prob\", \"correct\"]])\n",
        "else:\n",
        "    print(\"Either `batch_df` is not defined yet or `labels.csv` was not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b412c2c9",
      "metadata": {
        "id": "b412c2c9"
      },
      "source": [
        "## 10. Connecting Visual AI to MLOps (Optional Extension)\n",
        "\n",
        "To link this **Visual AI project** with your earlier **MLflow + Evidently** work:\n",
        "\n",
        "- Treat CLIP inference as a **model** and log:\n",
        "  - Parameters: model name, prompt set, device, etc.  \n",
        "  - Metrics: accuracy on labeled images, perâ€‘class accuracy.  \n",
        "  - Artifacts: a CSV with predictions, confusion matrix images.  \n",
        "\n",
        "- In a more advanced setup, you could:\n",
        "  - Integrate **image metadata** (component type, area, timestamp) with **sensor data** (temperature, pressure).  \n",
        "  - Build a joint dashboard: *\"show me all turbine images where pressure was above X.\"*  \n",
        "\n",
        "Below is a minimal example of logging a batch evaluation run to MLflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8f4110",
      "metadata": {
        "id": "5d8f4110"
      },
      "outputs": [],
      "source": [
        "# 10.1 Minimal MLflow logging example for the visual model (optional)\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Configure MLflow (here: local folder; change to remote server if needed)\n",
        "mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
        "mlflow.set_experiment(\"energy_plant_visual_ai_demo\")\n",
        "\n",
        "if \"batch_df\" in globals():\n",
        "    with mlflow.start_run(run_name=\"clip_visual_inference_batch\") as run:\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_images\", len(batch_df))\n",
        "        mlflow.log_param(\"label_set\", \", \".join(plain_labels))\n",
        "\n",
        "        # If we computed accuracy with ground truth above, log it too\n",
        "        if \"accuracy\" in globals():\n",
        "            mlflow.log_metric(\"top1_accuracy\", float(accuracy))\n",
        "\n",
        "        # Save predictions as CSV\n",
        "        out_csv = \"visual_predictions.csv\"\n",
        "        batch_df.to_json(\"visual_predictions.json\", orient=\"records\", indent=2)\n",
        "        batch_df.to_csv(out_csv, index=False)\n",
        "        mlflow.log_artifact(out_csv, artifact_path=\"predictions\")\n",
        "\n",
        "        print(\"Logged visual AI batch run with MLflow. Run ID:\", run.info.run_id)\n",
        "else:\n",
        "    print(\"Run the batch classification first to create `batch_df`.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645dbd91",
      "metadata": {
        "id": "645dbd91"
      },
      "source": [
        "## 11. Project Summary and Workshop Talking Points\n",
        "\n",
        "You now have a complete **Visual AI miniâ€‘project** for an energy plant context:\n",
        "\n",
        "- **Concept:** Classify plant components (turbine, valve, control panel, etc.) from images.  \n",
        "- **Model:** Openâ€‘source **CLIP** model from Hugging Face (`openai/clip-vit-base-patch32`).  \n",
        "- **Flow:**  \n",
        "  1. Load & prepare images  \n",
        "  2. Define prompts that reflect your plant components  \n",
        "  3. Run zeroâ€‘shot inference and inspect predictions  \n",
        "  4. Batchâ€‘score folders and (optionally) evaluate against labels  \n",
        "  5. Log results to MLflow for experiment tracking  \n",
        "\n",
        "### How to present this in your workshop\n",
        "\n",
        "- Position it as: **â€œYour first Visual AI pipeline without training a CNN from scratch.â€**  \n",
        "- Emphasize:\n",
        "  - Reâ€‘using **preâ€‘trained foundation models**  \n",
        "  - The importance of **good text prompts**  \n",
        "  - How this connects with **MLOps** and monitoring over time  \n",
        "\n",
        "From here, natural next steps are:\n",
        "- Fineâ€‘tuning a vision model on your own labeled plant dataset  \n",
        "- Combining imageâ€‘based predictions with sensor data for richer risk models  \n",
        "- Deploying this as a simple web app (e.g., with Gradio or FastAPI) for technicians to upload photos from the field.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
